{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VQ-UNET Experiment\n",
                "This notebook implements the VQ-UNET architecture for image reconstruction on the PASCAL VOC dataset. It includes data loading, model training, and evaluation steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Experiment Configuration:\n",
                        "DATA_ROOT: ./data\n",
                        "BATCH_SIZE: 16\n",
                        "IMAGE_SIZE: (128, 128)\n",
                        "CODEBOOK_SIZE: 512\n",
                        "COMMITMENT_COST: 0.25\n",
                        "LEARNING_RATE: 0.0001\n",
                        "NUM_EPOCHS: 25\n",
                        "WEIGHT_DECAY: 1e-05\n",
                        "VQ_LOSS_WEIGHT: 1.0\n",
                        "LOG_INTERVAL: 10\n",
                        "CHECKPOINT_DIR: checkpoints/\n",
                        "USE_CUDA: True\n",
                        "DEVICE: cpu\n",
                        "SEED: 42\n",
                        "Using device: cpu\n",
                        "Loading PASCAL VOC 2012 train set from ./data...\n",
                        "Loading PASCAL VOC 2012 val set from ./data...\n",
                        "Datasets loaded.\n",
                        "Model initialized.\n",
                        "Optimizer and criterion initialized.\n",
                        "Trainer initialized.\n",
                        "Checkpoint loaded: checkpoints/checkpoint_epoch_6.pth, starting from epoch 6\n",
                        "Starting training...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training Epoch 1/25:   0%|          | 0/358 [00:00<?, ?it/s]/Users/rd/miniforge3/envs/vqunet/lib/python3.13/site-packages/torch/utils/data/dataloader.py:685: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
                        "  warnings.warn(warn_msg)\n",
                        "Training Epoch 1/25:   3%|â–Ž         | 12/358 [00:28<13:52,  2.41s/it, Recon Loss=0.0882, VQ Loss=1.4199, Total Loss=1.5081]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/CODE/approach/my-vq-unet-project/src/trainer.py:41\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     38\u001b[39m recon_loss = \u001b[38;5;28mself\u001b[39m.criterion(reconstructions, targets)\n\u001b[32m     39\u001b[39m total_loss = recon_loss + \u001b[38;5;28mself\u001b[39m.vq_loss_weight * vq_loss\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m     44\u001b[39m running_recon_loss += recon_loss.item()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/vqunet/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/vqunet/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/vqunet/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "import torchvision.transforms as transforms\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "from src.dataset import VOCReconstructionDataset, get_transforms # Updated import\n",
                "from src.vq_unet_model import VQUNet\n",
                "from src.trainer import Trainer\n",
                "from configs.experiment_config import config # Correct way to import\n",
                "\n",
                "# Display config\n",
                "config.display()\n",
                "\n",
                "# Set device\n",
                "device = torch.device(config.DEVICE)\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Seed for reproducibility\n",
                "torch.manual_seed(config.SEED)\n",
                "if device == 'cuda':\n",
                "    torch.cuda.manual_seed_all(config.SEED)\n",
                "np.random.seed(config.SEED)\n",
                "\n",
                "# Data loading and transformation\n",
                "transform = get_transforms(image_size=config.IMAGE_SIZE)\n",
                "\n",
                "print(f\"Loading PASCAL VOC 2012 train set from {config.DATA_ROOT}...\")\n",
                "train_dataset = VOCReconstructionDataset(root=config.DATA_ROOT, year='2012', image_set='train', download=False, transform=transform)\n",
                "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
                "\n",
                "print(f\"Loading PASCAL VOC 2012 val set from {config.DATA_ROOT}...\")\n",
                "val_dataset = VOCReconstructionDataset(root=config.DATA_ROOT, year='2012', image_set='val', download=True, transform=transform)\n",
                "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
                "print(\"Datasets loaded.\")\n",
                "\n",
                "# Initialize model\n",
                "# Define encoder channel dimensions (example, can be configured)\n",
                "encoder_channel_dims = [64, 128, 256, 512] \n",
                "model = VQUNet(\n",
                "    in_channels=3, \n",
                "    out_channels=3, \n",
                "    codebook_size=config.CODEBOOK_SIZE,\n",
                "    encoder_channel_dims=encoder_channel_dims,\n",
                "    commitment_cost=config.COMMITMENT_COST\n",
                ").to(device)\n",
                "\n",
                "\n",
                "print(\"Model initialized.\")\n",
                "\n",
                "# Optimizer and Criterion\n",
                "optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
                "criterion = nn.MSELoss() # For reconstruction loss\n",
                "print(\"Optimizer and criterion initialized.\")\n",
                "\n",
                "# Initialize trainer\n",
                "trainer = Trainer(\n",
                "    model=model, \n",
                "    train_loader=train_loader, \n",
                "    val_loader=val_loader, \n",
                "    optimizer=optimizer, \n",
                "    criterion=criterion, \n",
                "    device=device, \n",
                "    num_epochs=config.NUM_EPOCHS, \n",
                "    checkpoint_dir=config.CHECKPOINT_DIR,\n",
                "    vq_loss_weight=config.VQ_LOSS_WEIGHT\n",
                ")\n",
                "print(\"Trainer initialized.\")\n",
                "# Load checkpoint if exists\n",
                "trainer.load_checkpoint(\"checkpoints/checkpoint_epoch_6.pth\") # Example checkpoint path\n",
                "\n",
                "# Start training\n",
                "print(\"Starting training...\")\n",
                "trainer.train()\n",
                "print(\"Training finished.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualize Results\n",
                "After training, we can visualize some of the results from the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize some results\n",
                "def visualize_results(model, data_loader, device, num_images=8):\n",
                "    model.eval()\n",
                "    images, _ = next(iter(data_loader)) # Get a batch of images\n",
                "    images = images.to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        reconstructions, vq_loss = model(images) # Model returns reconstructions and VQ loss\n",
                "    \n",
                "    images = images.cpu()\n",
                "    reconstructions = reconstructions.cpu()\n",
                "\n",
                "    # Denormalize for visualization if necessary (assuming Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)))\n",
                "    def denorm(tensor_img):\n",
                "        return tensor_img * 0.5 + 0.5\n",
                "\n",
                "    fig, ax = plt.subplots(2, num_images, figsize=(16, 4))\n",
                "    plt.suptitle(f'VQ Loss on this batch: {vq_loss.item():.4f}', fontsize=16)\n",
                "    for i in range(num_images):\n",
                "        if i < images.shape[0]: # Check if enough images in batch\n",
                "            original_img = denorm(images[i]).permute(1, 2, 0).numpy().clip(0,1)\n",
                "            reconstructed_img = denorm(reconstructions[i]).permute(1, 2, 0).numpy().clip(0,1)\n",
                "            \n",
                "            ax[0, i].imshow(original_img)\n",
                "            ax[0, i].set_title('Original')\n",
                "            ax[0, i].axis('off')\n",
                "            \n",
                "            ax[1, i].imshow(reconstructed_img)\n",
                "            ax[1, i].set_title('Reconstructed')\n",
                "            ax[1, i].axis('off')\n",
                "        else:\n",
                "            ax[0, i].axis('off')\n",
                "            ax[1, i].axis('off')\n",
                "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "print(\"Visualizing results from validation set...\")\n",
                "visualize_results(model, val_loader, device)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "vqunet",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
